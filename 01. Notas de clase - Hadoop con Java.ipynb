{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count en Java - HADOOP\n",
    "## Definición del problema\n",
    "Se desea contar la frecuencia de ocurrencia de palabras en un conjunto de documentos. Debido a los requerimientos de diseño (gran volúmen de datos y tiempos rápidos de respuesta) se desea implementar una arquitectura Big Data. **Se desea probar el código en la maquina local. El código debe ser escrito en Java.**\n",
    "\n",
    "A continuación se generarán tres archivos de prieba para probar el sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Preparación del directorio de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/e/Notas de clase/Ciencia de los grandes datos\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf input output*\n",
    "!mkdir input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text0.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text0.txt\n",
    "Analytics is the discovery, interpretation, and communication of meaningful patterns \n",
    "in data. Especially valuable in areas rich with recorded information, analytics relies \n",
    "on the simultaneous application of statistics, computer programming and operations research \n",
    "to quantify performance.\n",
    "\n",
    "Organizations may apply analytics to business data to describe, predict, and improve business \n",
    "performance. Specifically, areas within analytics include predictive analytics, prescriptive \n",
    "analytics, enterprise decision management, descriptive analytics, cognitive analytics, Big \n",
    "Data Analytics, retail analytics, store assortment and stock-keeping unit optimization, \n",
    "marketing optimization and marketing mix modeling, web analytics, call analytics, speech \n",
    "analytics, sales force sizing and optimization, price and promotion modeling, predictive \n",
    "science, credit risk analysis, and fraud analytics. Since analytics can require extensive \n",
    "computation (see big data), the algorithms and software used for analytics harness the most \n",
    "current methods in computer science, statistics, and mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text1.txt\n",
    "The field of data analysis. Analytics often involves studying past historical data to \n",
    "research potential trends, to analyze the effects of certain decisions or events, or to \n",
    "evaluate the performance of a given tool or scenario. The goal of analytics is to improve \n",
    "the business by gaining knowledge which can be used to make improvements or changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input/text2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input/text2.txt\n",
    "Data analytics (DA) is the process of examining data sets in order to draw conclusions \n",
    "about the information they contain, increasingly with the aid of specialized systems \n",
    "and software. Data analytics technologies and techniques are widely used in commercial \n",
    "industries to enable organizations to make more-informed business decisions and by \n",
    "scientists and researchers to verify or disprove scientific models, theories and \n",
    "hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de una solución Map-Reduce implementada en Hadoop y Java\n",
    "Para esto, solo se requiere crear las funciones para mapear (mapper) y para reducer (reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing WordCount.java\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount.java\n",
    "\n",
    "import java.io.IOException;\n",
    "\n",
    "/*\n",
    " * Esta clase permite separar una frase (texto)\n",
    " * en las palabras que lo conforman. La lista\n",
    " * resultante puede ser iterada en un ciclo for\n",
    " */\n",
    "import java.util.StringTokenizer;\n",
    "\n",
    "/*\n",
    " *\n",
    " * Librerias requeridas para ejecutar Hadoop\n",
    " *\n",
    " */\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "/*\n",
    " * Esta clase implementa el mapper y el reducer\n",
    " */\n",
    "public class WordCount {\n",
    "  \n",
    "  /* \n",
    "   * Esta es la clase que se encargará de hacer el mapeo de las\n",
    "   * palabras. Para ello necesita que tenga una funcion que se\n",
    "   * llame \"map\"\n",
    "   */\n",
    "  public static class TokenizerMapper\n",
    "       extends Mapper<Object, Text, Text, IntWritable>{\n",
    "       \n",
    "    // Implenteacion de un entero optimizado para hadoop\n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "\n",
    "    /* \n",
    "     * en esta variable se guarda cada palabra leida        \n",
    "     * del flujo de entrada\n",
    "     */     \n",
    "    private Text word = new Text();\n",
    "\n",
    "    /* \n",
    "     * Este es el mapper. Para cada palabra \n",
    "     * leída, emite el par <word, 1>\n",
    "     */\n",
    "    public void map(Object key,       // Clave\n",
    "                    Text value,       // La linea de texto\n",
    "                    Context context   // Aplicación que se esta ejecutando\n",
    "                    ) throws IOException, InterruptedException {\n",
    "                              \n",
    "      // Convierte la línea de texto en una lista de strings\n",
    "      StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "                              \n",
    "      // Ejecuta el ciclo para cada palabra \n",
    "      // de la lista de strings\n",
    "      while (itr.hasMoreTokens()) {\n",
    "        // obtiene la palabra\n",
    "        word.set(itr.nextToken());\n",
    "\n",
    "        // escribe la pareja <word, 1> \n",
    "        // al flujo de salida\n",
    "        context.write(word, one);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  public static class IntSumReducer\n",
    "       extends Reducer<Text,IntWritable,Text,IntWritable> {\n",
    "           \n",
    "    // Clase para imprimir un entero al flujo de salida       \n",
    "    private IntWritable result = new IntWritable();\n",
    "\n",
    "    // Esta función es llamada para reducir \n",
    "    // una lista de valores que tienen la misma clave\n",
    "    public void reduce(Text key,                      // clave\n",
    "                       Iterable<IntWritable> values,  // lista de valores\n",
    "                       Context context                // Aplicación que se esta ejecutando\n",
    "                       ) throws IOException, InterruptedException {\n",
    "        \n",
    "      // itera sobre la lista de valores, sumandolos\n",
    "      int sum = 0;\n",
    "      for (IntWritable val : values) {\n",
    "        sum += val.get();\n",
    "      }\n",
    "      result.set(sum);\n",
    "        \n",
    "      // escribe la pareja <word, valor> al flujo\n",
    "      // de salida\n",
    "      context.write(key, result);\n",
    "    }\n",
    "  }\n",
    "\n",
    "    \n",
    "  /*\n",
    "   * Se crea la aplicación en Hadoop y se ejecuta\n",
    "   */\n",
    "  public static void main(String[] args) throws Exception {\n",
    "    Configuration conf = new Configuration();\n",
    "    \n",
    "    /*\n",
    "     * El job corresponde a la aplicacion\n",
    "     */\n",
    "    Job job = Job.getInstance(conf, \"word count\");\n",
    "      \n",
    "    /*\n",
    "     * La clase que contiene el mapper y el reducer\n",
    "     */\n",
    "    job.setJarByClass(WordCount.class);\n",
    "      \n",
    "    /* \n",
    "     * Clase que implementa el mapper  \n",
    "     */ \n",
    "    job.setMapperClass(TokenizerMapper.class);\n",
    "      \n",
    "    /*\n",
    "     * El combiner es un reducer que se coloca a la salida\n",
    "     * del mapper para agilizar el computo\n",
    "     */\n",
    "    job.setCombinerClass(IntSumReducer.class);\n",
    "    \n",
    "    /*\n",
    "     * Clase que implementa el reducer\n",
    "     */\n",
    "    job.setReducerClass(IntSumReducer.class);\n",
    "      \n",
    "    /*\n",
    "     * Salida\n",
    "     */\n",
    "    job.setOutputKeyClass(Text.class);\n",
    "    job.setOutputValueClass(IntWritable.class);\n",
    "    \n",
    "    /*\n",
    "     * Formatos de entrada y salida\n",
    "     */\n",
    "    FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "   \n",
    "    // resultado de la ejecución.\n",
    "    System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Luego lo siguiente que se debe hacer es compilar el programa, de manera que se pueda ejecutar, para esto utilizamos el mismo hadoop, pasandole primero como clase el compilador de java \"javac\" y la clase a compilar. *(Si les lanza un error de **bad substitution** hacer caso omiso de esto, sin embargo investigar por qué lo lanza)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wm/hadoop-3.1.1/libexec/hadoop-functions.sh: line 2358: HADOOP_COM.SUN.TOOLS.JAVAC.MAIN_USER: bad substitution\r\n",
      "/home/wm/hadoop-3.1.1/libexec/hadoop-functions.sh: line 2453: HADOOP_COM.SUN.TOOLS.JAVAC.MAIN_OPTS: bad substitution\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop com.sun.tools.javac.Main WordCount.java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generar el jar con las clases compiladas adentro para que se puedan ejecutar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jar cf wc.jar WordCount*.class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Verificar la creación del jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wc.jar\r\n"
     ]
    }
   ],
   "source": [
    "!ls *.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Luego de generado el .jar ya s puede enviar al sistema hadoop para que sea ejecutado. **NOTA**: En este momento, el sistema hadoop está ejecutando en modo *standalone* es decir en un solo hilo, como en modo de prueba, sin embargo una ejecucion en un clustes real sería identico, el cambio viene en la configuración de hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wm/hadoop-3.1.1/etc/hadoop/core-site.xml\n",
      "-----------------------------------------------------------------------\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
      "<!--\n",
      "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "  you may not use this file except in compliance with the License.\n",
      "  You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "  Unless required by applicable law or agreed to in writing, software\n",
      "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "  See the License for the specific language governing permissions and\n",
      "  limitations under the License. See accompanying LICENSE file.\n",
      "-->\n",
      "\n",
      "<!-- Put site-specific property overrides in this file. -->\n",
      "\n",
      "<configuration>\n",
      "\n",
      "    <!--\n",
      "\n",
      "    <property>\n",
      "        <name>fs.defaultFS</name>\n",
      "        <value>hdfs://localhost:9000</value>\n",
      "    </property>\n",
      "\n",
      "    -->\n",
      "\n",
      "</configuration>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/home/wm/hadoop-3.1.1/etc/hadoop/hdfs-site.xml\n",
      "-----------------------------------------------------------------------\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
      "<!--\n",
      "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "  you may not use this file except in compliance with the License.\n",
      "  You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "  Unless required by applicable law or agreed to in writing, software\n",
      "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "  See the License for the specific language governing permissions and\n",
      "  limitations under the License. See accompanying LICENSE file.\n",
      "-->\n",
      "\n",
      "<!-- Put site-specific property overrides in this file. -->\n",
      "\n",
      "<configuration>\n",
      "    \n",
      "    <property>\n",
      "        <name>dfs.replication</name>\n",
      "        <value>1</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "        <name>dfs.namenode.name.dir</name>\n",
      "        <value>/mnt/e/hadoop_data_disk/name</value>\n",
      "    </property>\n",
      "    \n",
      "    <property>\n",
      "        <name>dfs.datanode.data.dir</name>\n",
      "        <value>/mnt/e/hadoop_data_disk/data</value>\n",
      "    </property>\n",
      "\n",
      "</configuration>\n"
     ]
    }
   ],
   "source": [
    "!echo ${HADOOP_HOME}/etc/hadoop/core-site.xml\n",
    "!echo \"-----------------------------------------------------------------------\"\n",
    "!cat $HADOOP_HOME/etc/hadoop/core-site.xml\n",
    "!echo \"\\n\"\n",
    "!echo \"\\n\"\n",
    "!echo ${HADOOP_HOME}/etc/hadoop/hdfs-site.xml\n",
    "!echo \"-----------------------------------------------------------------------\"\n",
    "!cat $HADOOP_HOME/etc/hadoop/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***POR FIN!!!! :)*** vamos a ejecutar nuestro contador de palabras en el sistema hadoop bajo la metodología Map-Reduce :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ejecuta el proceso para los archivos \n",
    "## de texto en el directorio input\n",
    "## Se puede usar la opción --loglevel {OFF, FATAL, ERROR, WARN, DEBUG, INFO, TRACE, ALL} asi:\n",
    "##\n",
    "##         !$HADOOP_HOME/bin/hadoop --loglevel ERROR jar wc.jar WordCount input output \n",
    "##\n",
    "## o modificar el archivo .bash_profile con la variable de entorno\n",
    "##\n",
    "##   export HADOOP_ROOT_LOGGER=\"ERROR,console\"\n",
    "##\n",
    "!$HADOOP_HOME/bin/hadoop jar wc.jar WordCount input output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Revisamos la carpeta output que es la salida de la ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS  part-r-00000\r\n"
     ]
    }
   ],
   "source": [
    "!ls output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DA)\t1\r\n",
      "(see\t1\r\n",
      "Analytics\t2\r\n",
      "Analytics,\t1\r\n",
      "Big\t1\r\n",
      "Data\t3\r\n",
      "Especially\t1\r\n",
      "Organizations\t1\r\n",
      "Since\t1\r\n",
      "Specifically,\t1\r\n"
     ]
    }
   ],
   "source": [
    "!cat output/part-r-00000 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "1. Ejecutar el conteo de palabras unicamente con el archivo text0.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop jar wc.jar WordCount input/text0.txt output.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS  part-r-00000\r\n"
     ]
    }
   ],
   "source": [
    "!ls output.1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(see\t1\r\n",
      "Analytics\t1\r\n",
      "Analytics,\t1\r\n",
      "Big\t1\r\n",
      "Data\t1\r\n",
      "Especially\t1\r\n",
      "Organizations\t1\r\n",
      "Since\t1\r\n",
      "Specifically,\t1\r\n",
      "algorithms\t1\r\n"
     ]
    }
   ],
   "source": [
    "!cat output.1/part-r-00000 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Mejorar el código para que logre organizar bien las palabras (limpiarlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## se limpia el directoroio de trabajo## se l \n",
    "!rm WordCount*.* *.jar\n",
    "!rm -rf output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'01. Notas de clase - Hadoop con Java.ipynb'   hadoop-3.1.1.tar.gz\r\n",
      " LICENSE\t\t\t\t       input\r\n",
      " README.md\t\t\t\t       jdk-8u181-linux-x64.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Modificar el codigo de la clase del mapper para que limpie las palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing WordCount.java\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount.java\n",
    "\n",
    "import java.io.IOException;\n",
    "\n",
    "/*\n",
    " * Esta clase permite separar una frase (texto)\n",
    " * en las palabras que lo conforman. La lista\n",
    " * resultante puede ser iterada en un ciclo for\n",
    " */\n",
    "import java.util.StringTokenizer;\n",
    "\n",
    "/*\n",
    " *\n",
    " * Librerias requeridas para ejecutar Hadoop\n",
    " *\n",
    " */\n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "\n",
    "/*\n",
    " * Esta clase implementa el mapper y el reducer\n",
    " */\n",
    "public class WordCount {\n",
    "  \n",
    "  /* \n",
    "   * Esta es la clase que se encargará de hacer el mapeo de las\n",
    "   * palabras. Para ello necesita que tenga una funcion que se\n",
    "   * llame \"map\"\n",
    "   */\n",
    "  public static class TokenizerMapper\n",
    "       extends Mapper<Object, Text, Text, IntWritable>{\n",
    "       \n",
    "    // Implenteacion de un entero optimizado para hadoop\n",
    "    private final static IntWritable one = new IntWritable(1);\n",
    "\n",
    "    /* \n",
    "     * en esta variable se guarda cada palabra leida        \n",
    "     * del flujo de entrada\n",
    "     */     \n",
    "    private Text word = new Text();\n",
    "\n",
    "    /* \n",
    "     * Este es el mapper. Para cada palabra \n",
    "     * leída, emite el par <word, 1>\n",
    "     */\n",
    "    public void map(Object key,       // Clave\n",
    "                    Text value,       // La linea de texto\n",
    "                    Context context   // Aplicación que se esta ejecutando\n",
    "                    ) throws IOException, InterruptedException {\n",
    "                              \n",
    "      // Convierte la línea de texto en una lista de strings\n",
    "      StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "                              \n",
    "      // Ejecuta el ciclo para cada palabra \n",
    "      // de la lista de strings\n",
    "      while (itr.hasMoreTokens()) {\n",
    "          \n",
    "          \n",
    "        // NOTA: AGREGADA PARTE PARA LIMPIAR LAS PALABRAS\n",
    "        // obtiene la palabra\n",
    "        word.set(itr.nextToken().toLowerCase().replaceAll(\"[^a-zA-Z0-9]\",\"\"));\n",
    "\n",
    "          \n",
    "        // escribe la pareja <word, 1> \n",
    "        // al flujo de salida\n",
    "        context.write(word, one);\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  public static class IntSumReducer\n",
    "       extends Reducer<Text,IntWritable,Text,IntWritable> {\n",
    "           \n",
    "    // Clase para imprimir un entero al flujo de salida       \n",
    "    private IntWritable result = new IntWritable();\n",
    "\n",
    "    // Esta función es llamada para reducir \n",
    "    // una lista de valores que tienen la misma clave\n",
    "    public void reduce(Text key,                      // clave\n",
    "                       Iterable<IntWritable> values,  // lista de valores\n",
    "                       Context context                // Aplicación que se esta ejecutando\n",
    "                       ) throws IOException, InterruptedException {\n",
    "        \n",
    "      // itera sobre la lista de valores, sumandolos\n",
    "      int sum = 0;\n",
    "      for (IntWritable val : values) {\n",
    "        sum += val.get();\n",
    "      }\n",
    "      result.set(sum);\n",
    "        \n",
    "      // escribe la pareja <word, valor> al flujo\n",
    "      // de salida\n",
    "      context.write(key, result);\n",
    "    }\n",
    "  }\n",
    "\n",
    "    \n",
    "  /*\n",
    "   * Se crea la aplicación en Hadoop y se ejecuta\n",
    "   */\n",
    "  public static void main(String[] args) throws Exception {\n",
    "    Configuration conf = new Configuration();\n",
    "    \n",
    "    /*\n",
    "     * El job corresponde a la aplicacion\n",
    "     */\n",
    "    Job job = Job.getInstance(conf, \"word count\");\n",
    "      \n",
    "    /*\n",
    "     * La clase que contiene el mapper y el reducer\n",
    "     */\n",
    "    job.setJarByClass(WordCount.class);\n",
    "      \n",
    "    /* \n",
    "     * Clase que implementa el mapper  \n",
    "     */ \n",
    "    job.setMapperClass(TokenizerMapper.class);\n",
    "      \n",
    "    /*\n",
    "     * El combiner es un reducer que se coloca a la salida\n",
    "     * del mapper para agilizar el computo\n",
    "     */\n",
    "    job.setCombinerClass(IntSumReducer.class);\n",
    "    \n",
    "    /*\n",
    "     * Clase que implementa el reducer\n",
    "     */\n",
    "    job.setReducerClass(IntSumReducer.class);\n",
    "      \n",
    "    /*\n",
    "     * Salida\n",
    "     */\n",
    "    job.setOutputKeyClass(Text.class);\n",
    "    job.setOutputValueClass(IntWritable.class);\n",
    "    \n",
    "    /*\n",
    "     * Formatos de entrada y salida\n",
    "     */\n",
    "    FileInputFormat.addInputPath(job, new Path(args[0]));\n",
    "    FileOutputFormat.setOutputPath(job, new Path(args[1]));\n",
    "   \n",
    "    // resultado de la ejecución.\n",
    "    System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compilar y motar en un jar el codigo modificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wm/hadoop-3.1.1/libexec/hadoop-functions.sh: line 2358: HADOOP_COM.SUN.TOOLS.JAVAC.MAIN_USER: bad substitution\r\n",
      "/home/wm/hadoop-3.1.1/libexec/hadoop-functions.sh: line 2453: HADOOP_COM.SUN.TOOLS.JAVAC.MAIN_OPTS: bad substitution\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop com.sun.tools.javac.Main WordCount.java\n",
    "!jar cf wc.jar WordCount*.class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ejecutar de nuevo el sistema hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop jar wc.jar WordCount input output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\t1\r\n",
      "about\t1\r\n",
      "aid\t1\r\n",
      "algorithms\t1\r\n",
      "analysis\t2\r\n",
      "analytics\t20\r\n",
      "analyze\t1\r\n",
      "and\t15\r\n",
      "application\t1\r\n",
      "apply\t1\r\n"
     ]
    }
   ],
   "source": [
    "!cat output/part-r-00000 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## se limpia el directoroio de trabajo\n",
    "!rm WordCount*.* *.jar\n",
    "!rm -rf input output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'01. Notas de clase - Hadoop con Java.ipynb'   hadoop-3.1.1.tar.gz\r\n",
      " LICENSE\t\t\t\t       jdk-8u181-linux-x64.tar.gz\r\n",
      " README.md\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
